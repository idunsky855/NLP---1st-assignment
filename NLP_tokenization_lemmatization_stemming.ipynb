{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaeec9c-56d1-4db8-8630-d687b945896e",
   "metadata": {},
   "source": [
    "# NLP - Tokenization, Lemmatization and Stemming\n",
    "\n",
    "## By: Idan Dunsky and Yaniv Kaveh Shtul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9879713f-b314-4dbe-8124-0207ea6d42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db96b94-81f5-4771-b650-b209491814c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/idandunsky/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/idandunsky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/idandunsky/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "file_encoding = 'latin-1' \n",
    "df = pd.read_csv('spam.csv',encoding=file_encoding)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936948ae-ffbe-4b93-a387-5155f992d314",
   "metadata": {},
   "source": [
    "## *Statistical Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173890d2-bfc0-40c2-b78f-f51e507580c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sms:  5572\n",
      "Number of spam messages:  747\n",
      "Number of ham messages:  4825\n"
     ]
    }
   ],
   "source": [
    "num_of_msg = df.shape[0]\n",
    "\n",
    "print(\"Total number of sms: \", num_of_msg)\n",
    "print(\"Number of spam messages: \",df[\"v1\"].value_counts()[\"spam\"])\n",
    "print(\"Number of ham messages: \",df[\"v1\"].value_counts()[\"ham\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868dbccc-ae23-4c42-b8f8-4df87e671030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage number of words per message:  15.60678391959799\n"
     ]
    }
   ],
   "source": [
    "word_count = sum([len(x.split(\" \")) for x in df[\"v2\"]])\n",
    "avg_num_of_words = word_count / num_of_msg\n",
    "\n",
    "print(\"Avarage number of words per message: \",avg_num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac6e77b-6461-40d6-acc3-d43e7045eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most common words are: \n",
      "1. \"i\", count: 2900\n",
      "2. \"to\", count: 2241\n",
      "3. \"you\", count: 2228\n",
      "4. \"a\", count: 1423\n",
      "5. \"the\", count: 1324\n"
     ]
    }
   ],
   "source": [
    "# Function to process text\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum()]\n",
    "    return filtered_tokens\n",
    "\n",
    "all_words = []\n",
    "for text in df['v2']:\n",
    "    all_words.extend(preprocess(text))\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "most_common_words = word_freq.most_common(5)\n",
    "\n",
    "print(\"The 5 most common words are: \")\n",
    "\n",
    "for i, word in enumerate(most_common_words):\n",
    "    print(f'{i+1}. \"{word[0]}\", count: {word[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42324733-eb2c-415b-a73e-b1ffac34089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that only appear once:  4077\n"
     ]
    }
   ],
   "source": [
    "words_with_one_occurrence = [word for word, count in word_freq.items() if count == 1]\n",
    "num_words_with_one_occurrence = len(words_with_one_occurrence)\n",
    "\n",
    "print(\"Number of words that only appear once: \", num_words_with_one_occurrence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de180f3-0d99-4aca-8cbd-8cd16db35943",
   "metadata": {},
   "source": [
    "## *Text Processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade63b8-9be6-417c-a818-255098c9c8c2",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c789772-8d29-44da-83d6-94ec90f3befa",
   "metadata": {},
   "source": [
    "* nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6600dd-5567-4684-9cea-58ac86100b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tok(l):\n",
    "\n",
    "    all_words_nltk = []\n",
    "    \n",
    "    for text in l:\n",
    "        all_words_nltk.extend(word_tokenize(text))\n",
    "\n",
    "    return [token.lower() for token in all_words_nltk if token.lower() not in stop_words and token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80fcf816-b66b-4d82-9de4-062ad4dce062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time complexity of nltk tokenization is:  0.2602710723876953\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "filtered_words_nltk = nltk_tok(df['v2'])\n",
    "\n",
    "total_time =  time.time() - start_time\n",
    "\n",
    "print(\"The time complexity of nltk tokenization is: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b0434-4d7e-40cc-a8c3-a0cc30321f72",
   "metadata": {},
   "source": [
    "* spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87a0010-1b81-4a0b-890a-2131ad66be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tok(l):\n",
    "    remove_punctuation_from_list(l)\n",
    "    \n",
    "    all_words_spacy = []\n",
    "    \n",
    "    for text in l:\n",
    "        all_words_spacy.extend(tokenizer(text))\n",
    "\n",
    "    return [token.text.lower() for token in all_words_spacy if not token.is_stop and not token.is_punct]\n",
    "\n",
    "\n",
    "def remove_punctuation_from_list(words):\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return [token.text for token in doc if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f59cd7-5827-4c91-8717-a27c9a0c45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time complexity of spaCy tokenization is:  0.8161511421203613\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "filtered_words_spacy = spacy_tok(df['v2'])\n",
    "\n",
    "total_time =  time.time() - start_time\n",
    "\n",
    "print(\"The time complexity of spaCy tokenization is: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58926182-6169-4a56-bf14-9e387d8638da",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc532293-735c-4b2f-8dff-335ff213f111",
   "metadata": {},
   "source": [
    "* nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fab0f2-816b-49c9-9fa0-e85caeaafda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_nltk(l):\n",
    "    return [lemmatizer.lemmatize(word) for word in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007a745c-c2e1-4872-b3f9-bfe0f39a0ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time complexity of nltk lemmatization is:  0.5794250965118408\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lemmitaized_nltk = lem_nltk(filtered_words_nltk)\n",
    "\n",
    "total_time =  time.time() - start_time\n",
    "\n",
    "print(\"The time complexity of nltk lemmatization is: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525dd16-1765-4775-97dd-4ba04881b7d3",
   "metadata": {},
   "source": [
    "* spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e916e82-74bd-4108-a6a7-3a76dd89dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_lemmatizer(text):\n",
    "    \n",
    "    lemmatized_spacy = []\n",
    "   \n",
    "    for word in text:\n",
    "        lemmatized_spacy.extend(nlp(word))\n",
    "        \n",
    "    return [token.lemma_ for token in lemmatized_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f221a-49ee-4fbd-854f-e5ac204f2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "start_time = time.time()\n",
    "        \n",
    "lemmatized_words = sp_lemmatizer(filtered_words_spacy)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"The time complexity of spacy lemmatization is: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e97dcd-1ab9-4bba-a588-454e15c02541",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d4339-ed71-4111-adf0-255034cf274d",
   "metadata": {},
   "source": [
    "* nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7db64c-13ea-48a4-97e9-442a99747413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemin_nltk(l):\n",
    "    return [snowball.stem(word) for word in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17deba66-8b63-4c5d-95ab-0544e7ebead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stem_nltk = stemin_nltk(lemmitaized_nltk)\n",
    "\n",
    "total_time =  time.time() - start_time\n",
    "\n",
    "print(\"The time complexity of nltk stemming is: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa408fa-0997-4406-97de-bcbe61ce58ce",
   "metadata": {},
   "source": [
    "* spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759ce07-d9ac-40e9-943e-0eb632dbab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy doesn't provide stemming tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531f4b3-5bea-41ed-abc1-1dcff691834a",
   "metadata": {},
   "source": [
    "#### Conclutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0657e1-d25a-45ad-a84f-74059ebf8c20",
   "metadata": {},
   "source": [
    "* Output Format: spaCy generally provides more user-friendly and integrated outputs, while NLTK's outputs often require additional processing.\n",
    "* Processing Speed: spaCy is way slower than NLTK, particularly for large datasets.\n",
    "* Language Support: spaCy has broader and more robust language support with pre-trained models available for multiple languages, while NLTK's language support is more limited and less comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a83a3-c00e-4dfa-a46d-888cf420e1d9",
   "metadata": {},
   "source": [
    "#### Statistics on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587aaf85-e70a-4f2e-a54d-18a89185aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq(l):\n",
    "        word_freq = Counter(l)\n",
    "        most_common_words = word_freq.most_common(5)\n",
    "        return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b09048-33d3-45e9-89f5-1c0f1ff7e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_words_with_one_occurrence(l):\n",
    "        word_freq = Counter(l)\n",
    "        words_with_one_occurrence = [word for word, count in word_freq.items() if count == 1]\n",
    "        num_words_with_one_occurrence = len(words_with_one_occurrence)\n",
    "\n",
    "        return num_words_with_one_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0725bc-c5c6-4663-bf6d-25ff2d90bc27",
   "metadata": {},
   "source": [
    "* nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdeea37-e676-4704-94ad-7e4655ff713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(stem_nltk)} words in nltk tokens list')\n",
    "\n",
    "print(\"Most 5 common words in lntk: \\n\")\n",
    "\n",
    "for i, word in enumerate(most_freq(stem_nltk)):\n",
    "    print(f'{i+1}. \"{word[0]}\", count: {word[1]}')\n",
    "\n",
    "print(\"\\nNumber of words that only appear once in lntk: \", num_of_words_with_one_occurrence(stem_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7469f18-a451-4a5e-b2f7-b7f20e044375",
   "metadata": {},
   "source": [
    "* spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29c720-3cca-42c8-b852-d0e43f176156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nThere are {len(filtered_words_spacy)} words in spaCy tokens list')\n",
    "\n",
    "print(\"Most 5 common words in spacy: \\n\")\n",
    "\n",
    "for i, word in enumerate(most_freq(filtered_words_spacy)):\n",
    "    print(f'{i+1}. \"{word[0]}\", count: {word[1]}')\n",
    "\n",
    "print(\"\\nNumber of words that only appear once in spaCy: \", num_of_words_with_one_occurrence(filtered_words_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c424777-9702-4106-aea5-4e4ff619d284",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11df4c3-345b-4619-a393-c4ae33bd2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL of the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "\n",
    "# Send a GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the main content text\n",
    "    # Wikipedia's main content is typically within <div> tags with the 'mw-parser-output' class\n",
    "    content_div = soup.find('div', class_='mw-parser-output')\n",
    "    \n",
    "    # Initialize an empty list to hold all text content\n",
    "    all_text = []\n",
    "\n",
    "    # Extract text from various elements\n",
    "    for element in content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote']):\n",
    "        all_text.append(element.get_text())\n",
    "\n",
    "    # Combine all text into a single string\n",
    "    wiki_text = '\\n'.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fdab60-59ea-45cd-9cf5-66540968a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_wiki = nltk_tok(wiki_text.split(\"\\n\"))\n",
    "lem_wiki = lem_nltk(tok_wiki)\n",
    "stem_wiki = stemin_nltk(lem_wiki)\n",
    "\n",
    "wiki_words = [y for x in wiki_text.split(\"\\n\") for y in x.split(\" \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388145c-21c4-4b20-9e00-d94773ad7fe4",
   "metadata": {},
   "source": [
    "#### Word Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d34118-bcab-407d-b9c8-f6c23618bef0",
   "metadata": {},
   "source": [
    "* Before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bab2c-7a54-4370-a239-4c7aeeff10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(wiki_words)} words in the wikipedia page before processing')\n",
    "\n",
    "print(\"Most 5 common words in the wikipedia page: \\n\")\n",
    "\n",
    "for i, word in enumerate(most_freq(wiki_words)):\n",
    "    print(f'{i+1}. \"{word[0]}\", count: {word[1]}')\n",
    "\n",
    "print(\"\\nNumber of words that only appear once in the wikipedia page: \", num_of_words_with_one_occurrence(wiki_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d01689-e472-42df-b9d7-b115fdb2c7b6",
   "metadata": {},
   "source": [
    "* After processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67e3f3-7970-4a9c-9f9d-347b59ec0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(lem_wiki)} words in the wikipedia page after processing')\n",
    "\n",
    "print(\"Most 5 common words in the wikipedia page: \\n\")\n",
    "\n",
    "for i, word in enumerate(most_freq(lem_wiki)):\n",
    "    print(f'{i+1}. \"{word[0]}\", count: {word[1]}')\n",
    "\n",
    "print(\"\\nNumber of words that only appear once in the wikipedia page: \", num_of_words_with_one_occurrence(lem_wiki))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
